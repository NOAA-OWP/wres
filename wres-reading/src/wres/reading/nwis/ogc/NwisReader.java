package wres.reading.nwis.ogc;

import java.io.IOException;
import java.io.InputStream;
import java.net.URI;
import java.net.URISyntaxException;
import java.time.Instant;
import java.time.ZoneId;
import java.time.ZoneOffset;
import java.util.ArrayList;
import java.util.Collections;
import java.util.HashMap;
import java.util.HashSet;
import java.util.List;
import java.util.Map;
import java.util.Objects;
import java.util.Set;
import java.util.SortedSet;
import java.util.StringJoiner;
import java.util.TimeZone;
import java.util.TreeSet;
import java.util.concurrent.ArrayBlockingQueue;
import java.util.concurrent.BlockingQueue;
import java.util.concurrent.CountDownLatch;
import java.util.concurrent.Future;
import java.util.concurrent.ThreadFactory;
import java.util.concurrent.ThreadPoolExecutor;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicBoolean;
import java.util.function.Function;
import java.util.function.IntPredicate;
import java.util.function.Supplier;
import java.util.stream.Collectors;
import java.util.stream.Stream;

import tools.jackson.databind.DeserializationFeature;
import tools.jackson.databind.ObjectMapper;

import com.github.benmanes.caffeine.cache.Cache;
import com.github.benmanes.caffeine.cache.Caffeine;
import okhttp3.Headers;
import org.apache.commons.lang3.concurrent.BasicThreadFactory;
import org.apache.commons.lang3.tuple.Pair;
import org.apache.http.client.utils.URIBuilder;
import org.jspecify.annotations.NonNull;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import tools.jackson.databind.json.JsonMapper;

import wres.config.DeclarationException;
import wres.config.DeclarationUtilities;
import wres.config.components.EvaluationDeclaration;
import wres.config.components.Source;
import wres.config.components.Variable;
import wres.datamodel.time.TimeSeries;
import wres.datamodel.time.TimeSeriesMetadata;
import wres.datamodel.time.TimeSeriesSlicer;
import wres.http.WebClient;
import wres.reading.DataSource;
import wres.reading.ReadException;
import wres.reading.ReaderUtilities;
import wres.reading.TimeChunker;
import wres.reading.TimeSeriesReader;
import wres.reading.TimeSeriesTuple;
import wres.reading.nwis.ogc.response.MonitoringLocation;
import wres.statistics.generated.GeometryTuple;
import wres.system.SystemSettings;

import wres.reading.nwis.ogc.response.NwisResponseReader;

/**
 * <p>Reads time-series data from the OGC-compliant USGS National Water Information System (NWIS) time-series web
 * services. Specifically, the supported services include the daily values service and the continuous values service,
 * which replaces the legacy instantaneous values service. The daily values service and its API is described here:
 *
 * <p><a href="https://api.waterdata.usgs.gov/ogcapi/v0/collections/daily">USGS NWIS DV Web Service</a>
 *
 * <p>The continuous values service and its API is described here:
 *
 * <p><a href="https://api.waterdata.usgs.gov/ogcapi/v0/collections/continuous">USGS NWIS CV Web Service</a>
 *
 * <p>The above links were last accessed: 20251125T12:00Z.
 *
 * <p>Implementation notes:
 *
 * <p>Time-series are chunked by feature and year ranges. The underlying format reader is a {@link NwisResponseReader}.
 * The NWIS supports paginated reads. All the pages generated by a single request are read in a single thread. Parallel
 * reads are instead achieved by chunking the data into prescribed time intervals and a separate thread reads each chunk.
 * Note that the NWIS DV service does not provide fully qualified datetimes on a standard timeline. Rather, it supplies
 * dates that implicitly represent a datetime of midnight in local time. As such, feature-specific timing information
 * must be obtained from a separate monitoring_locations endpoint. This endpoint does not itself provide fully qualified
 * time zone information, rather an abbreviated time zone offset, together with a daylight savings indicator, which is
 * ambiguous. This limits the shape of reading, in practice. For example, it is not possible to read multiple features
 * than span more than one time zone in a single read as the timing information must be supplied as a sidecar to each
 * read via the {@link DataSource}. Conversely, the CV service provides offset datetimes, which are fully qualified.
 *
 * @author James Brown
 */

public class NwisReader implements TimeSeriesReader
{
    /** Logger. */
    private static final Logger LOGGER = LoggerFactory.getLogger( NwisReader.class );

    /** A format reader, instantiated without any location metadata. */
    private static final NwisResponseReader GEOJSON_READER = NwisResponseReader.of();

    /** Message string. */
    private static final String USGS_NWIS = "USGS National Water Information System";

    /** The HTTP response codes considered to represent no data. */
    private static final IntPredicate NO_DATA_PREDICATE = c -> c == 404;

    /** The HTTP response codes considered to represent an error to be thrown. */
    private static final IntPredicate ERROR_RESPONSE_PREDICATE = c -> !( c >= 200 && c < 300 );

    /** Number of time-series values per page. */
    private static final int DEFAULT_PAGE_SIZE = 10000;

    /** Re-used string. */
    public static final String WRES_NWIS_API_KEY = "wres.nwisApiKey";

    /** Cache of location metadata for re-use. */
    private final Cache<@NonNull String, LocationMetadata> locationCache =
            Caffeine.newBuilder()
                    .maximumSize( 100 ) // 100, arbitrarily
                    .build();

    /** For reading feature metadata. */
    private static final ObjectMapper OBJECT_MAPPER =
            JsonMapper.builder()
                      .configure( DeserializationFeature.FAIL_ON_NULL_FOR_PRIMITIVES, true )
                      .build();

    /** Mapping between time zone short names and formal IANA time zone names. This is highly brittle but stems from
     * the NWIS DV service providing ambiguous information about time zones via abbreviated names. */
    private static final Map<String, String> TIMEZONE_MAP;

    /** The time chunker. */
    private final TimeChunker timeChunker;

    // Populate the timezone map for time zones supported by USGS NWIS. If some are missing, expect a runtime exception,
    // which will need to be mitigated by adding more. Most are supported out of the box, but some are not and these are
    // added below.
    static
    {
        TIMEZONE_MAP = new HashMap<>();
        TIMEZONE_MAP.putAll( ZoneId.SHORT_IDS );
        TIMEZONE_MAP.put( "EST", "America/New_York" );
        TIMEZONE_MAP.put( "HST", "Pacific/Honolulu" );
        TIMEZONE_MAP.put( "MST", "America/Denver" );
        TIMEZONE_MAP.put( "AKST", "America/Anchorage" );
    }

    /** Re-used string.*/
    private static final String ITEMS = "items";

    /** Pair declaration, which is used to chunk requests. Null if no chunking is required. */
    private final EvaluationDeclaration declaration;

    /** A thread pool to process web requests. */
    private final ThreadPoolExecutor executor;

    /**
     * @param declaration the declaration, which is used to perform chunking of a data source
     * @param systemSettings the system settings
     * @param timeChunker the time chunker
     * @return an instance
     * @throws NullPointerException if either input is null
     */

    public static NwisReader of( EvaluationDeclaration declaration,
                                 SystemSettings systemSettings,
                                 TimeChunker timeChunker )
    {
        return new NwisReader( declaration, systemSettings, timeChunker );
    }

    @Override
    public Stream<TimeSeriesTuple> read( DataSource dataSource )
    {
        Objects.requireNonNull( dataSource );

        LOGGER.debug( "Preparing requests for the {} that chunk the time-series data by feature and "
                      + "time range.", USGS_NWIS );

        return this.read( dataSource, this.getDeclaration() );
    }

    /**
     * This implementation is equivalent to calling {@link NwisResponseReader#read(DataSource, InputStream)}.
     * @param dataSource the data source, required
     * @param stream the input stream, required
     * @return the stream of time-series
     * @throws NullPointerException if the dataSource is null
     * @throws ReadException if the reading fails for any other reason
     */

    @Override
    public Stream<TimeSeriesTuple> read( DataSource dataSource, InputStream stream )
    {
        LOGGER.debug( "Discovered an existing stream, assumed to be from the {}. Passing through to an underlying "
                      + "GeoJSON reader.", USGS_NWIS );

        return GEOJSON_READER.read( dataSource, stream );
    }

    /**
     * @return the pair declaration, possibly null
     */

    private EvaluationDeclaration getDeclaration()
    {
        return this.declaration;
    }

    /**
     * @return the thread pool executor
     */

    private ThreadPoolExecutor getExecutor()
    {
        return this.executor;
    }

    /**
     * Reads the data source by forming separate requests by feature and time range.
     *
     * @param dataSource the data source
     * @param declaration the declaration used for chunking
     * @throws NullPointerException if either input is null
     */

    private Stream<TimeSeriesTuple> read( DataSource dataSource, EvaluationDeclaration declaration )
    {
        Objects.requireNonNull( dataSource );
        Objects.requireNonNull( declaration );

        // The features
        Set<GeometryTuple> geometries = DeclarationUtilities.getFeatures( declaration );

        Set<String> features = ReaderUtilities.getFeatureNamesFor( geometries, dataSource );

        // Date ranges
        Set<Pair<Instant, Instant>> dateRanges = this.timeChunker.get();

        // Combine the features and date ranges to form the chunk boundaries
        Set<Pair<String, Pair<Instant, Instant>>> chunks = new HashSet<>();
        for ( String nextFeature : features )
        {
            // Add the USGS qualifier if not currently qualified
            if ( !nextFeature.contains( "-" ) )
            {
                nextFeature = "USGS-" + nextFeature;
            }

            for ( Pair<Instant, Instant> nextDates : dateRanges )
            {
                Pair<String, Pair<Instant, Instant>> nextChunk = Pair.of( nextFeature, nextDates );
                chunks.add( nextChunk );
            }
        }

        // Get the lazy supplier of time-series data, which supplies one series per chunk of data
        Supplier<TimeSeriesTuple> supplier = this.getTimeSeriesSupplier( dataSource,
                                                                         Collections.unmodifiableSet( chunks ) );

        // Generate a stream of time-series. Nothing is read here. Rather, as part of a terminal operation on this
        // stream, each pull will read through to the supplier, then in turn to the data provider, and finally to
        // the data source.
        return Stream.generate( supplier )
                     // Finite stream, proceeds while a time-series is returned
                     .takeWhile( Objects::nonNull )
                     .onClose( () -> {
                         LOGGER.debug( "Detected a stream close event. Closing dependent resources." );
                         this.getExecutor()
                             .shutdownNow();
                     } );
    }

    /**
     * Returns a time-series supplier from the inputs.
     *
     * @param dataSource the data source
     * @param chunks the data chunks to iterate
     * @return a time-series supplier
     */

    private Supplier<TimeSeriesTuple> getTimeSeriesSupplier( DataSource dataSource,
                                                             Set<Pair<String, Pair<Instant, Instant>>> chunks )
    {
        LOGGER.debug( "Creating a time-series supplier to supply one time-series for each of these {} chunks: {}.",
                      chunks.size(),
                      chunks );

        SortedSet<Pair<String, Pair<Instant, Instant>>> mutableChunks = new TreeSet<>( chunks );

        // The size of this queue is equal to the setting for simultaneous web client threads so that we can 1. get
        // quick feedback on exception (which requires a small queue) and 2. allow some requests to go out prior to
        // get-one-response-per-submission-of-one-ingest-task
        int concurrentCount = this.getExecutor()
                                  .getMaximumPoolSize();
        BlockingQueue<Future<List<TimeSeriesTuple>>> results = new ArrayBlockingQueue<>( concurrentCount );

        // The size of this latch is for reason (2) above
        CountDownLatch startGettingResults = new CountDownLatch( concurrentCount );

        // Cached time-series to return
        List<TimeSeriesTuple> cachedSeries = new ArrayList<>();

        // Is true to continue looking for time-series
        AtomicBoolean proceed = new AtomicBoolean( true );

        // Create a supplier that returns a time-series once complete
        return () -> {

            // Clean up before sending the null sentinel, which terminates the stream
            // New rows to increment
            while ( proceed.get() )
            {
                // Cached series from an earlier iteration? If so, return it
                if ( !cachedSeries.isEmpty() )
                {
                    return cachedSeries.remove( 0 );
                }

                // Submit the next chunk if not already submitted
                if ( !mutableChunks.isEmpty() )
                {
                    Pair<String, Pair<Instant, Instant>> nextChunk = mutableChunks.first();
                    mutableChunks.remove( nextChunk );

                    // Create the inner data source for the chunk
                    DataSource innerSource = this.getChunkedDataSource( nextChunk, dataSource );
                    LocationMetadata locationMetadata = this.getLocationMetadata( nextChunk.getLeft(), innerSource );

                    // Log the metadata, but pass through the metadata cache, which now contains this metadata
                    LOGGER.debug( "Acquired the following location metadata from NWIS for feature {}: {}.",
                                  nextChunk.getLeft(),
                                  locationMetadata );

                    NwisResponseReader reader = NwisResponseReader.of( this.locationCache );

                    LOGGER.debug( "Submitting a reading task for chunk, {}.", innerSource );

                    // Get the next time-series as a future
                    Future<List<TimeSeriesTuple>> future = this.getExecutor()
                                                               .submit( () -> this.readAllPages( innerSource,
                                                                                                 reader ) );

                    results.add( future );
                }

                // Check that all is well with previously submitted tasks, but only after a handful have been
                // submitted. This means that an exception should propagate relatively shortly after it occurs with the
                // read task. It also means after the creation of a handful of tasks, we only create one after a
                // previously created one has been completed, fifo/lockstep.
                startGettingResults.countDown();
                List<TimeSeriesTuple> result = ReaderUtilities.getTimeSeries( results,
                                                                              startGettingResults,
                                                                              USGS_NWIS );

                cachedSeries.addAll( result );

                // Still some chunks to request or results to return?
                proceed.set( !mutableChunks.isEmpty()
                             || !results.isEmpty()
                             || cachedSeries.size() > 1 );

                LOGGER.debug( "Continuing to iterate chunks of data ({}) because some chunks were yet to be submitted "
                              + "({}) or some results were yet to be retrieved ({}) or some results are cached and "
                              + "awaiting return ({}).",
                              proceed.get(),
                              !mutableChunks.isEmpty(),
                              !results.isEmpty(),
                              cachedSeries.size() > 1 );

                // Return a result if there is one
                if ( !cachedSeries.isEmpty() )
                {
                    return cachedSeries.remove( 0 );
                }
            }

            // Null sentinel to close stream
            return null;
        };
    }

    /**
     * Reads all pages of data from a paginated stream, consolidating event values across pages that belong to a single
     * time-series, as needed.
     *
     * @param dataSource the data source with one or more pages
     * @param reader the reader
     * @return the time-series
     * @throws ReadException if the time-series could not be read for any reason
     */

    private List<TimeSeriesTuple> readAllPages( DataSource dataSource,
                                                NwisResponseReader reader )
    {
        if ( LOGGER.isDebugEnabled() )
        {
            LOGGER.debug( "Reading all pages of data from: {}.", dataSource.uri() );
        }

        List<TimeSeriesTuple> firstPage = this.readOnePage( dataSource, reader );
        List<TimeSeriesTuple> allPages = new ArrayList<>( firstPage );

        // Another pages to read?
        for ( TimeSeriesTuple nextTuple : firstPage )
        {
            if ( nextTuple.getDataSource()
                          .hasNextPage() )
            {
                // Currently, the next page must be adjusted to enforce the format requirement. Bug in NWIS?
                URI nextPageUri = nextTuple.getDataSource()
                                           .nextPage();
                URIBuilder builder = new URIBuilder( nextPageUri );
                builder.addParameter( "f", "json" );
                try
                {
                    DataSource newSource = dataSource.toBuilder()
                                                     .uri( builder.build() )
                                                     .build();
                    List<TimeSeriesTuple> nextPage = this.readAllPages( newSource, reader );
                    allPages.addAll( nextPage );

                    if ( LOGGER.isDebugEnabled() )
                    {
                        LOGGER.debug( "Detected a next page in data source: {}. The URI of the next page is: {}.",
                                      dataSource,
                                      nextPageUri );
                    }
                }
                catch ( URISyntaxException e )
                {
                    throw new ReadException( "While reading from the " + USGS_NWIS + ", failed to "
                                             + "adjust a base URI to add the format specification.", e );
                }
            }
            else if ( LOGGER.isDebugEnabled() )
            {
                LOGGER.debug( "Detected no next page in data source: {}.", dataSource );
            }
        }

        return this.consolidateTimeSeries( allPages, dataSource );
    }

    /**
     * Reads one page from a data source.
     * @param dataSource the data source
     * @param reader the reader
     * @return the time-series
     * @throws ReadException if the page could not be read for any reason
     */

    private List<TimeSeriesTuple> readOnePage( DataSource dataSource,
                                               NwisResponseReader reader )
    {
        // Unpack an HTTP 429 response code and report
        Function<WebClient.ClientResponse, String> errorUnpacker = response ->
        {
            if ( response.getStatusCode() == 429 )
            {
                String extra;

                if ( Objects.isNull( System.getProperty( WRES_NWIS_API_KEY ) ) )
                {
                    extra = " No API key (system property: nwis.wresApiKey) was discovered. It is strongly "
                            + "recommended that you acquire an API key from the USGS "
                            + "(https://api.waterdata.usgs.gov/signup/) and supply this key to the WRES using the "
                            + "system property, wres.nwisApiKey, which will increase your rate limit.";
                }
                else
                {
                    extra = " An API key (system property: nwis.wresApiKey) was discovered. As such, the rate limit "
                            + "you have exceeded is already a boosted rate limit and no further action can be taken. "
                            + "The rate limit will reset after a short period.";
                }

                return "Encountered an HTTP 429 Error, which happens when you request too much data from the "
                       + USGS_NWIS + " within a short time period." + extra;
            }

            return "";
        };

        // Get the input stream and read from it
        try ( WebClient.ClientResponse response = ReaderUtilities.getResponseFromWebSource( dataSource.uri(),
                                                                                            NO_DATA_PREDICATE,
                                                                                            ERROR_RESPONSE_PREDICATE,
                                                                                            errorUnpacker,
                                                                                            null ) )
        {
            if ( Objects.nonNull( response ) )
            {
                // Log rate limit info.
                this.logRateLimits( response.getHeaders() );

                return reader.read( dataSource, response.getResponse() )
                             .toList(); // Terminal
            }

            return List.of();
        }
        catch ( IOException e )
        {
            throw new ReadException( "Failed to read from the " + USGS_NWIS + ".", e );
        }
    }

    /**
     * Logs usage information and progress towards the rate limit.
     *
     * @param headers the response headers that contain the usage information
     */

    private void logRateLimits( Headers headers )
    {
        // Check the rate limiting headers
        String limit = headers.get( "X-RateLimit-Limit" );
        String remaining = headers.get( "X-RateLimit-Remaining" );

        if ( Objects.nonNull( limit )
             && Objects.nonNull( remaining ) )
        {
            try
            {
                int limitInt = Integer.parseInt( limit );
                int remainingInt = Integer.parseInt( remaining );

                LOGGER.debug( "The NWIS rate limit is {} and there are {} requests remaining within the current period "
                              + "before rate limiting begins.", limitInt, remainingInt );

                // Print a warning when 10% or fewer requests are remaining, repeated every 10 requests
                if ( remainingInt % 10 == 0
                     && ( remainingInt / ( double ) limitInt ) <= 0.1 )
                {
                    LOGGER.warn( "Requests to the {} are rate limited. The rate limit is {} and there are only "
                                 + "{} requests remaining before rate limiting begins, which will result in an HTTP "
                                 + "429 error and a failed evaluation. The rate limit resets periodically. For more "
                                 + "information on acceptable use of NWIS services, please visit: "
                                 + "https://api.waterdata.usgs.gov/docs/ogcapi/", USGS_NWIS, limitInt, remainingInt );
                }
            }
            catch ( NumberFormatException e )
            {
                LOGGER.debug( "Failed to parse the expected rate limiting headers from an NWIS response." );
            }
        }
    }

    /**
     * Consolidates events that belong to time-series with common metadata into single time-series.
     *
     * @param toConsolidate the time-series to consolidate
     * @param dataSource the data source
     * @return the consolidated time-series
     */

    private List<TimeSeriesTuple> consolidateTimeSeries( List<TimeSeriesTuple> toConsolidate,
                                                         DataSource dataSource )
    {
        Map<TimeSeriesMetadata, List<TimeSeries<Double>>> singleValuedMapped =
                toConsolidate.stream()
                             .map( TimeSeriesTuple::getSingleValuedTimeSeries )
                             .collect( Collectors.groupingBy( TimeSeries::getMetadata ) );

        return singleValuedMapped.values()
                                 .stream()
                                 .map( TimeSeriesSlicer::consolidate )
                                 .map( t -> TimeSeriesTuple.ofSingleValued( t, dataSource ) )
                                 .toList();
    }

    /**
     * Returns a chunked {@link DataSource} for prescribed chunk boundaries and existing (unchunked) data source.
     * @param chunk the chunk
     * @param dataSource the unchunked data source
     * @return the chunked data source
     */

    private DataSource getChunkedDataSource( Pair<String, Pair<Instant, Instant>> chunk, DataSource dataSource )
    {
        URI nextUri = this.getUriForChunk( dataSource.source()
                                                     .uri(),
                                           dataSource,
                                           chunk.getRight(),
                                           chunk.getLeft() );

        LOGGER.debug( "Created a URI to request a chunk of data: {}.", nextUri );

        return dataSource.toBuilder()
                         .uri( nextUri )
                         .build();
    }

    /**
     * Attempts to populate the time zone information for a given feature. Any daylight saving adjustment must be
     * accounted for at the lowest level of reading as Daylight Saving Time is, by definition, datetime dependent and,
     * hence, requires the datetime for a specific event.
     *
     * @param featureId the feature
     * @param dataSource the data source
     * @return the adjusted data source
     * @throws ReadException if the required time zone offset could not be determined
     */

    private LocationMetadata getLocationMetadata( String featureId, DataSource dataSource )
    {
        LocationMetadata locationMetadata = this.getLocationMetadataFromCacheOrNwis( featureId, dataSource );

        if ( !dataSource.uri()
                        .getPath()
                        .contains( "daily" ) )
        {
            LOGGER.debug( "Adding a default time zone of UTC." );
            return locationMetadata.toBuilder()
                                   .zoneOffset( ZoneOffset.UTC )
                                   .build();
        }

        return locationMetadata;
    }

    /**
     * Attempts to retrieve the location metadata from a local cache, else from NWIS.
     *
     * @param featureId the feature
     * @param dataSource the data source
     * @return the time zone information
     * @throws ReadException if the required time zone shorthand could not be determined
     */

    private LocationMetadata getLocationMetadataFromCacheOrNwis( String featureId, DataSource dataSource )
    {
        LocationMetadata metadata = this.locationCache.getIfPresent( featureId );

        if ( Objects.nonNull( metadata ) )
        {
            LOGGER.debug( "Returning location metadata from the cache for feature {}.", featureId );

            return metadata;
        }

        // Attempt to discover the time zone information using the monitoring-locations endpoint
        URI baseUri = dataSource.uri();
        URIBuilder uriBuilder = new URIBuilder( baseUri );
        String path = baseUri.getPath();
        path = path.replaceAll( "(?<=collections/)(.*)(?=/items)", "monitoring-locations" );
        StringJoiner pathBuilder = new StringJoiner( "/" );
        pathBuilder.add( path );
        if ( !path.endsWith( ITEMS ) )
        {
            pathBuilder.add( ITEMS ); // NOSONAR
        }

        pathBuilder.add( featureId ); // Add the feature identifier

        uriBuilder.setPath( pathBuilder.toString() )
                  .clearParameters()
                  .addParameter( "f", "json" )
                  .addParameter( "lang", "en-US" );

        Source source = dataSource.source();

        LocationMetadata addMeToCache;

        try ( InputStream featureMetadata = ReaderUtilities.getByteStreamFromWebSource( uriBuilder.build(),
                                                                                        NO_DATA_PREDICATE,
                                                                                        ERROR_RESPONSE_PREDICATE,
                                                                                        null,
                                                                                        null ) )
        {
            if ( Objects.isNull( featureMetadata )
                 && Objects.nonNull( source.timeZoneOffset() ) )
            {
                LOGGER.warn( "When reading data from the {}, failed to identify the time zone information for "
                             + "geographic feature, '{}', from {}. However, discovered a declared 'time_zone_offset' "
                             + "of {}. This offset will be applied when reading data.",
                             USGS_NWIS,
                             featureId,
                             uriBuilder.build(),
                             source.timeZoneOffset() );
                addMeToCache = new LocationMetadata( null, null, source.timeZoneOffset() );
            }
            else if ( Objects.isNull( featureMetadata ) )
            {
                throw new ReadException( "When reading data from the "
                                         + USGS_NWIS
                                         + ", failed to identify the time zone information for geographic feature, '"
                                         + featureId
                                         + "' from "
                                         + uriBuilder.build()
                                         + ". Furthermore, there was no declared 'time_zone_offset'. Please declare "
                                         + "the 'time_zone_offset' to clarify the timing information for this data "
                                         + "source and try again." );
            }
            else
            {
                addMeToCache = this.getLocationMetadata( featureMetadata, dataSource, featureId );
            }

            this.locationCache.put( featureId, addMeToCache );

            return addMeToCache;
        }
        catch ( NullPointerException | URISyntaxException | IOException e )
        {
            throw new ReadException( "Unable to read feature metadata for feature '"
                                     + featureId
                                     + "', which is required to determine "
                                     + "the time zone information for this feature. If this issue persists, the "
                                     + "feature metadata may not be available and you should instead declare the "
                                     + "'time_zone_offset' explicitly.", e );
        }
    }

    /**
     * Reads the feature metadata from the input
     * @param featureMetadata the feature metadata stream
     * @param dataSource the data source
     * @param featureId the feature identifier
     * @return the time zone information
     * @throws IOException if the reading failed for any reason
     */

    private LocationMetadata getLocationMetadata( InputStream featureMetadata,
                                                  DataSource dataSource,
                                                  String featureId )
            throws IOException
    {
        Source source = dataSource.source();

        MonitoringLocation feature = OBJECT_MAPPER.readValue( featureMetadata.readAllBytes(),
                                                              MonitoringLocation.class );

        Objects.requireNonNull( feature.getProperties(), "Unable to read the feature metadata for "
                                                         + "feature '" + featureId + "'." );

        String zoneId = feature.getProperties()
                               .getTimeZoneAbbreviation();

        // Time zone information available
        if ( Objects.nonNull( zoneId ) )
        {
            if ( Objects.nonNull( source.timeZoneOffset() ) )
            {
                LOGGER.warn( "When reading data from the {} for geographic feature, '{}', discovered a declared "
                             + "'time_zone_offset' of {}. This offset will be ignored because the data declared its "
                             + "own time zone shorthand of '{}'. If you intended to shift the times to allow for "
                             + "pairing, then please declare a 'time_shift' instead.",
                             USGS_NWIS,
                             featureId,
                             source.timeZoneOffset(),
                             zoneId );
            }

            // If no daylight savings, use the raw offset, else the full time zone
            if ( Boolean.FALSE.equals( dataSource.source()
                                                 .daylightSavings() )  // User declared to ignore
                 || !"Y".equalsIgnoreCase( feature.getProperties()
                                                  .getUsesDaylightSavings() ) )
            {
                TimeZone timeZone = TimeZone.getTimeZone( zoneId );
                int offsetSeconds = timeZone.getRawOffset() / 1000;
                ZoneOffset zoneOffset = ZoneOffset.ofTotalSeconds( offsetSeconds );

                LOGGER.debug( "Using a time zone offset for feature {} as follows: {}",
                              featureId,
                              zoneOffset );

                return new LocationMetadata( feature.getGeometry(), null, zoneOffset );
            }
            else
            {
                // This is not the recommended way to acquire a formal time zone, but the NWIS response does not
                // provide a proper designation of a time zone using the IANA Time Zone standard, such as
                // "America/New_York", rather it uses a shorthand, such as "EST", which does not account for
                // local rules, such as daylight savings
                String formalName = TIMEZONE_MAP.get( zoneId );

                if ( Objects.isNull( formalName ) )
                {
                    throw new ReadException( "When attempting to read the time zone information for feature "
                                             + featureId
                                             + ", could not identify the formal IANA Time Zone Database time zone for "
                                             + "shorthand '"
                                             + zoneId
                                             + "'. Please declare the 'time_zone_offset' instead or request "
                                             + "that the time zone information is supported for shorthand '"
                                             + zoneId
                                             + "'." );
                }

                TimeZone timeZone = TimeZone.getTimeZone( formalName );

                LOGGER.debug( "Using full time zone information for feature {} as follows: {}",
                              featureId,
                              timeZone );

                return new LocationMetadata( feature.getGeometry(), timeZone, null );
            }
        }
        else
        {
            throw new IOException( "Failed to read the 'time_zone_abbreviation' for feature " + featureId + "." );
        }
    }

    /**
     * Get a URI for a given date range and feature.
     *
     * <p>Expecting a USGS URI like this:
     * <a href="https://api.waterdata.usgs.gov/ogcapi/v0/collections/">https://api.waterdata.usgs.gov/ogcapi/v0/collections/</a></p>
     *
     * @param baseUri the base uri associated with this source
     * @param dataSource the data source for which to create a URI
     * @param range the range of dates (from left to right)
     * @param featureName the features to include in the request URI
     * @return a URI suitable to get the data from WRDS API
     */

    private URI getUriForChunk( URI baseUri,
                                DataSource dataSource,
                                Pair<Instant, Instant> range,
                                String featureName )
    {
        Objects.requireNonNull( baseUri );
        Objects.requireNonNull( range );
        Objects.requireNonNull( featureName );
        Objects.requireNonNull( range.getLeft() );
        Objects.requireNonNull( range.getRight() );

        if ( !baseUri.toString()
                     .toLowerCase()
                     .contains( "collections/" )
             && LOGGER.isWarnEnabled() )
        {
            LOGGER.warn( "Expected a URI like 'https://api.waterdata.usgs.gov/ogcapi/v0/collections/' but got {}.",
                         baseUri );
        }

        // Qualify the last part of the path, if needed
        String baseEnd = baseUri.getPath()
                                .toLowerCase();
        if ( !baseEnd.endsWith( ITEMS ) )
        {
            URIBuilder builder = new URIBuilder( baseUri );
            String separator = "";
            if ( !baseEnd.endsWith( "/" ) )
            {
                separator = "/";
            }

            String path = baseUri.getPath() + separator + ITEMS;

            builder.setPath( path );
            try
            {
                baseUri = builder.build();
            }
            catch ( URISyntaxException e )
            {
                throw new ReadException( "While reading time-series data from the "
                                         + USGS_NWIS
                                         + ", failed to adjust a base URI to add the /items path element.", e );
            }
        }

        Map<String, String> urlParameters = this.getUrlParameters( range,
                                                                   featureName,
                                                                   dataSource );
        return ReaderUtilities.getUriWithParameters( baseUri,
                                                     urlParameters );
    }

    /**
     * Specific to USGS NWIS API, get date range url parameters
     * @param range the date range to set parameters for
     * @param featureName The USGS site code
     * @param dataSource The data source from which this request came
     * @return the key/value parameters
     * @throws NullPointerException When arg or value enclosed inside arg is null
     */

    private Map<String, String> getUrlParameters( Pair<Instant, Instant> range,
                                                  String featureName,
                                                  DataSource dataSource )
    {
        LOGGER.trace( "Called getUrlParameters with {}, {}, {}",
                      range,
                      featureName,
                      dataSource );

        Objects.requireNonNull( range );
        Objects.requireNonNull( featureName );
        Objects.requireNonNull( dataSource );
        Objects.requireNonNull( range.getLeft() );
        Objects.requireNonNull( range.getRight() );
        Objects.requireNonNull( dataSource.getVariable() );
        Objects.requireNonNull( dataSource.getVariable()
                                          .name() );

        // The start datetime needs to be one second later than the next
        // period's end datetime, or we request duplicates.
        // This follows the "pools are inclusive/exclusive" convention of WRES.
        // For some reason, 1 to 999 milliseconds are not enough.
        Instant startDateTime = range.getLeft()
                                     .plusSeconds( 1 );
        Map<String, String> urlParameters = new HashMap<>( dataSource.source()
                                                                     .parameters() );

        String parameterCodes = this.getParameterCodes( dataSource.getVariable() );
        urlParameters.put( "f", "json" );
        urlParameters.put( "lang", "en-US" );

        // For efficiency, acquire the location metadata from the monitoring locations endpoint
        urlParameters.put( "skipGeometry", "true" );

        urlParameters.put( "limit", Integer.toString( DEFAULT_PAGE_SIZE ) );
        urlParameters.put( "properties", "id,time_series_id,monitoring_location_id,statistic_id,time,"
                                         + "value,unit_of_measure" );
        urlParameters.put( "parameter_code", parameterCodes );
        urlParameters.put( "monitoring_location_id", featureName );

        String time = startDateTime + "/" + range.getRight();
        urlParameters.put( "time", time );

        String apiKey = System.getProperty( WRES_NWIS_API_KEY );

        if ( this.hasApiKey() )
        {
            LOGGER.debug( "Discovered a user-supplied API key for the {}, which will be added to the request "
                          + "using the 'api_key' parameter.", USGS_NWIS );
            urlParameters.put( "api_key", apiKey );
        }

        return Collections.unmodifiableMap( urlParameters );
    }

    /**
     * @return whether an API key has been defined
     */

    private boolean hasApiKey()
    {
        String apiKey = System.getProperty( WRES_NWIS_API_KEY );

        return Objects.nonNull( apiKey );
    }

    /**
     * @param variable the variable
     * @return the parameter codes
     */
    private String getParameterCodes( Variable variable )
    {
        StringJoiner start = new StringJoiner( "," )
                .add( variable.name() );

        if ( !variable.aliases()
                      .isEmpty() )
        {
            variable.aliases()
                    .forEach( start::add );
            LOGGER.debug( "Added the following aliased variable names for variable {}: {}",
                          variable.name(),
                          variable.aliases() );
        }

        return start.toString();
    }

    /**
     * Hidden constructor.
     * @param declaration the optional declaration, which is used to perform chunking of a data source
     * @param systemSettings the system settings
     * @throws DeclarationException if the project declaration is invalid for this source type
     * @throws NullPointerException if any input is null
     */

    private NwisReader( EvaluationDeclaration declaration,
                        SystemSettings systemSettings,
                        TimeChunker timeChunker )
    {
        Objects.requireNonNull( declaration );
        Objects.requireNonNull( systemSettings );
        Objects.requireNonNull( timeChunker );

        this.declaration = declaration;
        this.timeChunker = timeChunker;

        ThreadFactory webClientFactory = BasicThreadFactory.builder()
                                                           .namingPattern( "USGS NWIS Reading Thread %d" )
                                                           .build();

        // Use a queue with as many places as client threads
        BlockingQueue<Runnable> webClientQueue =
                new ArrayBlockingQueue<>( systemSettings.getMaximumWebClientThreads() );
        this.executor = new ThreadPoolExecutor( systemSettings.getMaximumWebClientThreads(),
                                                systemSettings.getMaximumWebClientThreads(),
                                                systemSettings.getPoolObjectLifespan(),
                                                TimeUnit.MILLISECONDS,
                                                webClientQueue,
                                                webClientFactory );

        // Because of use of latch and queue below, rejection should not happen.
        this.executor.setRejectedExecutionHandler( new ThreadPoolExecutor.AbortPolicy() );
    }

}
